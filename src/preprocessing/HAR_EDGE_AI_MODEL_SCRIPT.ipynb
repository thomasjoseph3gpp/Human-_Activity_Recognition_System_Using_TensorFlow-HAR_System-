{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77832853-1805-4da9-9f9d-fede5eb5d111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc8f99f3-33f8-4914-828b-0126643b9619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Step 1: Creating Edge AI Project Structure\n",
      "============================================================\n",
      "data\\raw\n",
      "data\\processed\n",
      "data\\cache\n",
      "models\\tflite\n",
      "models\\onnx\n",
      "models\\checkpoints\n",
      "src\\preprocessing\n",
      "src\\quantization\n",
      "src\\evaluation\n",
      "config\n",
      "docs\n",
      "deployment\\firmware\n",
      "deployment\\benchmarks\n",
      "tests\\unit\n",
      "tests\\integration\n",
      "Project created at : C:\\Python\\Edge_AI\\edge_har_project\n",
      "============================================================\n",
      "Step 2: Downloading the necessary dataset from the given url\n",
      "============================================================\n",
      "Downloading dataset...\n",
      "dataset downloaded to:edge_har_project\\data\\raw\\human+activity+recognition+using+smartphones.zip\n",
      "\n",
      "============================================================\n",
      "Loading Dataset..\n",
      "============================================================\n",
      "Dataset Loaded...\n",
      "X_train shape:(7352, 561)\n",
      "y_train shape:(7352,)\n",
      "X_test shape:(2947, 561)\n",
      "y_test shape:(2947,)\n",
      "Metaata saved to :edge_har_project\\data\\raw\\dataset_metadata.json\n",
      "edge_har_project\\data\\raw\\human+activity+recognition+using+smartphones.zip removed succesfully.\n",
      "Cleanup complete\n",
      "\n",
      "============================================================\n",
      "Creating and analyzing the model\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">35,968</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚          \u001b[38;5;34m35,968\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  â”‚           \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   â”‚             \u001b[38;5;34m198\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,246</span> (149.40 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m38,246\u001b[0m (149.40 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">38,246</span> (149.40 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m38,246\u001b[0m (149.40 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Train Model\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Training model\n",
      "==================================================\n",
      "Epoch 1/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.5599 - loss: 1.0617 - val_accuracy: 0.8320 - val_loss: 0.3322 - learning_rate: 0.0100\n",
      "Epoch 2/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8052 - loss: 0.4709 - val_accuracy: 0.8843 - val_loss: 0.3435 - learning_rate: 0.0100\n",
      "Epoch 3/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8047 - loss: 0.4719 - val_accuracy: 0.8968 - val_loss: 0.2515 - learning_rate: 0.0100\n",
      "Epoch 4/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8507 - loss: 0.3747 - val_accuracy: 0.8137 - val_loss: 0.3992 - learning_rate: 0.0100\n",
      "Epoch 5/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8530 - loss: 0.4028 - val_accuracy: 0.9070 - val_loss: 0.2682 - learning_rate: 0.0100\n",
      "Epoch 6/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8699 - loss: 0.3687 - val_accuracy: 0.9230 - val_loss: 0.2457 - learning_rate: 0.0100\n",
      "Epoch 7/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8739 - loss: 0.3360 - val_accuracy: 0.9131 - val_loss: 0.2196 - learning_rate: 0.0100\n",
      "Epoch 8/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8926 - loss: 0.2742 - val_accuracy: 0.8979 - val_loss: 0.2480 - learning_rate: 0.0100\n",
      "Epoch 9/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8991 - loss: 0.2677 - val_accuracy: 0.9274 - val_loss: 0.2120 - learning_rate: 0.0100\n",
      "Epoch 10/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8822 - loss: 0.3052 - val_accuracy: 0.8880 - val_loss: 0.2783 - learning_rate: 0.0100\n",
      "Epoch 11/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8924 - loss: 0.2659 - val_accuracy: 0.8140 - val_loss: 0.5328 - learning_rate: 0.0100\n",
      "Epoch 12/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8066 - loss: 0.4655 - val_accuracy: 0.8626 - val_loss: 0.3566 - learning_rate: 0.0100\n",
      "Epoch 13/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8079 - loss: 0.4581 - val_accuracy: 0.7357 - val_loss: 0.7012 - learning_rate: 0.0100\n",
      "Epoch 14/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8476 - loss: 0.3973 - val_accuracy: 0.8246 - val_loss: 0.4283 - learning_rate: 0.0100\n",
      "Epoch 15/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8952 - loss: 0.2836 - val_accuracy: 0.9091 - val_loss: 0.2876 - learning_rate: 0.0050\n",
      "Epoch 16/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9045 - loss: 0.2558 - val_accuracy: 0.9240 - val_loss: 0.2527 - learning_rate: 0.0050\n",
      "Epoch 17/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9195 - loss: 0.2266 - val_accuracy: 0.9182 - val_loss: 0.2499 - learning_rate: 0.0050\n",
      "Epoch 18/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9213 - loss: 0.2188 - val_accuracy: 0.9040 - val_loss: 0.2981 - learning_rate: 0.0050\n",
      "Epoch 19/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9174 - loss: 0.2291 - val_accuracy: 0.9335 - val_loss: 0.2076 - learning_rate: 0.0050\n",
      "Epoch 20/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9270 - loss: 0.2113 - val_accuracy: 0.8965 - val_loss: 0.3023 - learning_rate: 0.0050\n",
      "Epoch 21/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9175 - loss: 0.2108 - val_accuracy: 0.8795 - val_loss: 0.3376 - learning_rate: 0.0050\n",
      "Epoch 22/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9177 - loss: 0.2197 - val_accuracy: 0.8887 - val_loss: 0.2917 - learning_rate: 0.0050\n",
      "Epoch 23/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9283 - loss: 0.2083 - val_accuracy: 0.8870 - val_loss: 0.3166 - learning_rate: 0.0050\n",
      "Epoch 24/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9263 - loss: 0.2020 - val_accuracy: 0.9213 - val_loss: 0.2436 - learning_rate: 0.0050\n",
      "Epoch 25/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9412 - loss: 0.1588 - val_accuracy: 0.9077 - val_loss: 0.2747 - learning_rate: 0.0025\n",
      "Epoch 26/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9403 - loss: 0.1740 - val_accuracy: 0.9019 - val_loss: 0.2637 - learning_rate: 0.0025\n",
      "Epoch 27/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9398 - loss: 0.1605 - val_accuracy: 0.8918 - val_loss: 0.3122 - learning_rate: 0.0025\n",
      "Epoch 28/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9355 - loss: 0.1746 - val_accuracy: 0.8755 - val_loss: 0.3514 - learning_rate: 0.0025\n",
      "Epoch 29/50\n",
      "\u001b[1m230/230\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9391 - loss: 0.1667 - val_accuracy: 0.8884 - val_loss: 0.3010 - learning_rate: 0.0025\n",
      "\n",
      "============================================================\n",
      "Model Evaluation\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Model Evaluation\n",
      "==================================================\n",
      "Test Accuracy:0.9335\n",
      "Test Loss:0.2076\n",
      "\u001b[1m93/93\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           Walking       0.93      0.98      0.95       496\n",
      "  Walking upstairs       0.96      0.94      0.95       471\n",
      "Walking Downstairs       0.97      0.94      0.95       420\n",
      "           Sitting       0.81      0.95      0.88       491\n",
      "          Standing       0.95      0.88      0.91       532\n",
      "            Laying       1.00      0.93      0.96       537\n",
      "\n",
      "          accuracy                           0.93      2947\n",
      "         macro avg       0.94      0.93      0.93      2947\n",
      "      weighted avg       0.94      0.93      0.93      2947\n",
      "\n",
      "\n",
      "============================================================\n",
      "Edge Performance Analysis\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Edge Performance Analysis\n",
      "==================================================\n",
      "2MB constraint:pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average_Inference Time:8.18 ms\n",
      "100 ms constraint:pass\n",
      "\n",
      "============================================================\n",
      "Save Model\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Saving Models for Deployment\n",
      "==================================================\n",
      "Keras model saved to :edge_har_project\\models\\checkpoints\\har_model.h5\n",
      "Training history saved:edge_har_project\\models\\checkpoints\\training_history.json\n",
      "\n",
      "============================================================\n",
      "Quantize Model for Edge\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Model Quantization(INT8)\n",
      "==================================================\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\thoma\\AppData\\Local\\Temp\\tmpiwiwp28s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\thoma\\AppData\\Local\\Temp\\tmpiwiwp28s\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\thoma\\AppData\\Local\\Temp\\tmpiwiwp28s'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 561), dtype=tf.float32, name='keras_tensor_5')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 6), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1505346579216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1505346581136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1505346574032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1505346582288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1505346581520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1505346583440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to:edge_har_project\\models\\tflite\\har_edge_model_int8.tflite\n",
      "Model size: 42.8 KB\n",
      "\n",
      "============================================================\n",
      "Testing TFLite Model\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "TFLite Model Testing\n",
      "==================================================\n",
      "TFLite Accuracy(first100 samples):0.9700\n",
      "\n",
      "============================================================\n",
      "Creating Deployment Report\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Deployment Readiness Report\n",
      "==================================================\n",
      "DEPLOYMENT SUMMARY\n",
      "Model Size:37.3KB(INT8)\n",
      "Inference Summary:8.18 ms\n",
      "Test Accuracy:0.9335\n",
      "TFLite Accuracy:0.9700\n",
      "\n",
      " Edge Constraints\n",
      "2MB SIZE:pass\n",
      "100ms inference:pass\n",
      "Ready for edge deployment\n",
      "\n",
      " Full Report Saved to: edge_har_project\\deployment\\deployment_report.json\n",
      "\n",
      "============================================================\n",
      "EDGE AI PIPELINE COMPLETED..\n",
      "============================================================\n",
      "\n",
      " Edge AI Model for motion detection is available at: C:\\Python\\Edge_AI\\edge_har_project\n",
      "\n",
      "Model files created:\n",
      "  âœ“ Keras model: models/checkpoints/har_model.h5\n",
      "  âœ“ TFLite model: models/tflite/har_edge_model_int8.tflite\n",
      "  âœ“ Config: config/project.json\n",
      "  âœ“ Report: deployment/deployment_report.json\n",
      "  âœ“ Dataset metadata: data/raw/dataset_metadata.json\n",
      "\n",
      "ğŸ“Š Model Performance Summary:\n",
      "  Test Accuracy: 0.9335\n",
      "  TFLite Accuracy: 0.9700\n",
      "  Model Size: 37.3 KB (INT8)\n",
      "  Inference Time: 8.18 ms\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import requests\n",
    "\n",
    "cwd = r\"C:\\Python\\Edge_AI\\edge_har_project\"\n",
    "os.chdir(r\"C:\\Python\\Edge_AI\")\n",
    "def create_edge_project(base_dir:str = 'edge_har_project') ->Path:\n",
    "    \"\"\"\n",
    "    Created standard Edge AI dirs and sub dirs\n",
    "    \"\"\"\n",
    "    base_path  =Path(base_dir)\n",
    "    folders = [   \n",
    "        #Data Pipeline\n",
    "        base_path/'data'/'raw',\n",
    "        base_path/'data'/'processed',\n",
    "        base_path/'data'/'cache',\n",
    "\n",
    "        #Models\n",
    "\n",
    "        base_path/'models'/'tflite',\n",
    "        base_path/'models'/'onnx',\n",
    "        base_path/'models'/'checkpoints',\n",
    "\n",
    "        #Source code\n",
    "\n",
    "        base_path/'src'/'preprocessing',\n",
    "        base_path/'src'/'quantization',\n",
    "        base_path/'src'/'evaluation',\n",
    "\n",
    "        #Config and Docs\n",
    "\n",
    "        base_path/'config',\n",
    "        base_path/'docs',\n",
    "\n",
    "        #Deployment\n",
    "\n",
    "        base_path/'deployment'/'firmware',\n",
    "        base_path/'deployment'/'benchmarks',\n",
    "\n",
    "        # Tests\n",
    "\n",
    "        base_path/'tests'/'unit',\n",
    "        base_path/'tests'/'integration'\n",
    "    ]\n",
    "\n",
    "    for folder in folders:\n",
    "        folder.mkdir(parents = True,exist_ok = True)\n",
    "        print(f\"{folder.relative_to(base_path)}\")\n",
    "    create_essential_files(base_path)\n",
    "    print(f\"Project created at : {base_path.absolute()}\")\n",
    "    return base_path\n",
    "\n",
    "def create_essential_files(base_path:Path):\n",
    "    '''creates only essential configuration files'''\n",
    "    config = {\n",
    "            \"project\": {\n",
    "                    \"name\":\"Edge HAR\",\n",
    "                    \"version\": \"1.0.0\"\n",
    "            },\n",
    "        \"defaults\": {\n",
    "            \"quantization\": \"int8\",\n",
    "            \"frame_size\":128,\n",
    "            \"sampling_rate\":25,\n",
    "            \"overlap\":0.5,\n",
    "            \"normalization\":\"z-score\",\n",
    "            \"activities\":[\"walking\",\"sitting\",\"standing\",\"stairs\"]\n",
    "        },\n",
    "        \"edge_constraints\": {\n",
    "            \"max_model_size_mb\": 2,\n",
    "            \"max_inference_time_ms\": 100, # Must respond quickly\n",
    "            \"max_power_consumption_mw\": 500,  # Battery constraints\n",
    "            \"target_devices\": [\"raspberry_pi\", \"arduino\", \"android\"]\n",
    "    },\n",
    "\n",
    "         \"preprocessing\": {\n",
    "        \"filter_type\": \"lowpass\",     # Remove noise\n",
    "        \"filter_cutoff_hz\": 5,        # Keep only human movement frequencies\n",
    "        \"remove_gravity\": True        # Separate gravity from movement\n",
    "    },\n",
    "    \n",
    "    \"model_architecture\": "model_architecture": {
        "type": "ann",                // Artificial Neural Network (MLP)
        "layers": 5,                  // Including Input and Dropout
        "trainable_layers": 3,        // Only Dense layers have parameters
        "parameters": 38246,          // Exact: 35,968 + 2,080 + 198 = 38,246
        "input_shape": [561]          // 1D vector of 561 features
    },
    "        \n",
    "        \n",
    "    }\n",
    "    config_path = base_path/\"config\"/\"project.json\"\n",
    "    config_path.parent.mkdir(parents = True, exist_ok = True)\n",
    "    config_path.write_text(json.dumps(config, indent =2))\n",
    "\n",
    "    gitignore = \"\"\"#Edge AI - \n",
    "    # Only system/temporary files excluded\n",
    "\n",
    "# Python cache\n",
    "__pycache__/\n",
    "*.pyc\n",
    "\n",
    "# IDE files\n",
    ".vscode/\n",
    ".idea/\n",
    "\n",
    "# OS files\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Virtual environments\n",
    "venv/\n",
    ".env\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "\"\"\"\n",
    "\n",
    "    (base_path/\".gitignore\").write_text(gitignore)\n",
    "    readme = \"\"\"# Edge AI Project\n",
    "\n",
    "    ## Quick Start\n",
    "1. Place raw data in `data/raw/`\n",
    "2. Run preprocessing: `python src/preprocessing/process.py`\n",
    "3. Train model: See notebooks/\n",
    "4. Deploy: `python deployment/build_firmware.py`\n",
    "\n",
    "## Edge Considerations\n",
    "- Use int8 quantization for deployment\n",
    "- Test on actual edge hardware\n",
    "- Monitor memory usage during inference\n",
    "\"\"\"\n",
    "\n",
    "                            \n",
    "\n",
    "                            \n",
    "    (base_path/\"config\"/\"project.json\").write_text(\n",
    "    json.dumps(config,indent = 2)\n",
    "    )\n",
    "    \n",
    "\n",
    "    (base_path/\"README.md\").write_text(readme)\n",
    "\n",
    "\n",
    "\n",
    "def load_har_data_uci_format(data_dir:Path):\n",
    "    \"\"\"\n",
    "    Load UCI HAR dataset from the extracted folder structure\n",
    "    Return (X_train,Y_train),(X_test,Y_test)\n",
    "    \"\"\"\n",
    "\n",
    "    #Loading Train Data\n",
    "    X_train = np.loadtxt(data_dir/\"train\"/\"X_train.txt\")\n",
    "    y_train = np.loadtxt(data_dir/\"train\"/\"y_train.txt\").astype(int) - 1\n",
    "\n",
    "    #Loading the test data\n",
    "    X_test = np.loadtxt(data_dir/\"test\"/\"X_test.txt\")\n",
    "    y_test = np.loadtxt(data_dir/\"test\"/\"y_test.txt\").astype(int) - 1\n",
    "    print(f\"Dataset Loaded...\")\n",
    "    print(f\"X_train shape:{X_train.shape}\")\n",
    "    print(f\"y_train shape:{y_train.shape}\")\n",
    "    print(f\"X_test shape:{X_test.shape}\")\n",
    "    print(f\"y_test shape:{y_test.shape}\")\n",
    "    return (X_train,y_train), (X_test,y_test)\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self,obj):\n",
    "        if isinstance(obj,np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj,np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder,self).default(obj)\n",
    "\n",
    "def create_dataset_metadata(data_dir: Path):\n",
    "    \"\"\"Creating metadata about the dataset\"\"\"\n",
    "    metadata = {\n",
    "        \"dataset\":\"UCI HAR\",\n",
    "        \"structure\": \"train/test split\",\n",
    "        \"features\": 561,\n",
    "        \"classes\":6,\n",
    "        \"activity_labels\": {\n",
    "            0:\"walking\",\n",
    "            1:\"walking_upstairs\",\n",
    "            2:\"walking_downstairs\",\n",
    "            3:\"sitting\",\n",
    "            4:\"standing\",\n",
    "            5:\"laying\"\n",
    "        },\n",
    "        \"statistics\": {\n",
    "            \"train_samples\": 7352,\n",
    "            \"test_samples\": 2947,\n",
    "            \"total_samples\":10299,\n",
    "            \"sampling_rate\":50,\n",
    "            \"window_size\":128,\n",
    "            \"overlap\":0.5\n",
    "        }\n",
    "    }\n",
    "    metadata_path = data_dir/\"dataset_metadata.json\"\n",
    "    metadata_path.write_text(json.dumps(metadata,indent = 2, cls = NumpyEncoder))\n",
    "    print(f\"Metaata saved to :{metadata_path}\")\n",
    "    return metadata\n",
    "def cleanup_dataset(data_dir:Path):\n",
    "    \"\"\"Remove large zip files to save space\"\"\"\n",
    "    files_to_remove = [\n",
    "        data_dir/\"human+activity+recognition+using+smartphones.zip\",\n",
    "        data_dir/\"UCI HAR Dataset.zip\",\n",
    "        data_dir/\"UCI HAR Dataset.names\"\n",
    "    ]\n",
    "    for file in files_to_remove:\n",
    "        if file.exists():\n",
    "            file.unlink()\n",
    "            print(f\"{file} removed succesfully.\")\n",
    "    print(\"Cleanup complete\")\n",
    "def create_tiny_ann(input_dim = 561, num_classes = 6):\n",
    "    \"\"\"Create tiny ANN for edge deployment\"\"\"\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape = (input_dim,)),\n",
    "        keras.layers.Dense(64,activation = 'relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(32,activation = 'relu'),\n",
    "        keras.layers.Dense(num_classes,activation = 'softmax')\n",
    "    ])\n",
    "    return model\n",
    "def train_model(model,X_train,y_train,X_test,y_test,epochs = 50):\n",
    "    \"\"\"Train model with edge parameters\"\"\"\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"Training model\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = 0.01),\n",
    "        loss = \"sparse_categorical_crossentropy\",\n",
    "        metrics  = [\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    #Callbacks for better training\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor = \"val_loss\",\n",
    "            patience = 10,\n",
    "            restore_best_weights = True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = \"val_loss\",\n",
    "            factor = 0.5,\n",
    "            patience = 5,\n",
    "            min_lr = 1e-6\n",
    "        )\n",
    "    ]\n",
    "    #Train the model\n",
    "    history = model.fit(\n",
    "        X_train,y_train,\n",
    "        validation_data =(X_test,y_test),\n",
    "        epochs = epochs,\n",
    "        batch_size = 32,\n",
    "        callbacks = callbacks,\n",
    "        verbose = 1\n",
    "    )\n",
    "    return history, model\n",
    "def evaluate_model(model,X_test,y_test):\n",
    "    \"\"\"Evaluate model performace\"\"\"\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"Model Evaluation\")\n",
    "    print(\"=\"*50)\n",
    "    #Standard Evaluation\n",
    "    test_loss, test_acc = model.evaluate(X_test,y_test,verbose = 0)\n",
    "    test_loss = float(test_loss)\n",
    "    test_acc = float(test_acc)\n",
    "    print(f\"Test Accuracy:{test_acc:.4f}\")\n",
    "    print(f\"Test Loss:{test_loss:.4f}\")\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    y_pred = np.argmax(model.predict(X_test),axis = 1)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test,y_pred,target_names = [\n",
    "        \"Walking\",\"Walking upstairs\",\"Walking Downstairs\",\"Sitting\",\"Standing\",\"Laying\"\n",
    "    ]))\n",
    "    return test_acc, test_loss\n",
    "\n",
    "def analyze_edge_performance(model,X_sample):\n",
    "    \"\"\"Analyze model for edge deployment constraints\"\"\"\n",
    "    import time\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"Edge Performance Analysis\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    #Model size analysis\n",
    "    params = model.count_params()\n",
    "    fp32_size_KB = params*4/1024\n",
    "    int8_size_KB = params/1024\n",
    "    print(f\"2MB constraint:\"\"pass\" if fp32_size_KB<2048 and int8_size_KB <2048 else f\"2MB constraint:\"\"fail\")\n",
    "\n",
    "    #Inference speed test\n",
    "    times = []\n",
    "    batch_size = 10\n",
    "    num_batches =10\n",
    "    total_samples = num_batches * batch_size\n",
    "    for i in range(10):\n",
    "        batch = X_sample[i*batch_size:(i+1)*batch_size]\n",
    "        start = time.perf_counter()\n",
    "        model.predict(batch,verbose = 0)\n",
    "        end = time.perf_counter()\n",
    "        times.append(((end-start)*1000)/batch_size)\n",
    "    avg_inference = float(np.mean(times))\n",
    "    print(f\"Average_Inference Time:{avg_inference:.2f} ms\")\n",
    "    #print(f\"100 ms constraint:\" \"pass\" if avg_inference<100 else f\"100 ms constraint:\" \"fail\")\n",
    "    print(f\"100 ms constraint:\" \"pass\" if avg_inference<100 else f\"100 ms constraint:\" \"fail\")\n",
    "    return int(params),float(avg_inference)\n",
    "def quantize_model_for_edge(model, X_train,project_path):\n",
    "    \"\"\"Quantize model to INT8 for edge deployment\"\"\"\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"Model Quantization(INT8)\")\n",
    "    print(\"=\"*50)\n",
    "    def representative_dataset():\n",
    "        for i in range(1,100):\n",
    "            yield[X_train[i:i+1].astype(np.float32)]\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "    try:\n",
    "        tflite_model = converter.convert()\n",
    "        tflite_path  =project_path/\"models\"/\"tflite\"/\"har_edge_model_int8.tflite\"\n",
    "        tflite_path.parent.mkdir(parents = True, exist_ok = True)\n",
    "        tflite_path.write_bytes(tflite_model)\n",
    "        print(f\"Quantized model saved to:{tflite_path}\")\n",
    "        print(f\"Model size: {len(tflite_model)/1024:.1f} KB\")\n",
    "        return tflite_model,tflite_path\n",
    "    except Exception as e:\n",
    "        print(f\"Quantization failed:{e}\")\n",
    "        return None, None\n",
    "\n",
    "def test_tflite_model(tflite_path, X_test,y_test):\n",
    "    \"Test the quantized TFLite model \"\"\"\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"TFLite Model Testing\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Load TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path = str(tflite_path))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    #Get input/output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Test accuracy\n",
    "    correct = 0\n",
    "    total = len(X_test)\n",
    "    test_samples = min(100,total) # test on first 100 samples\n",
    "\n",
    "    for i in range(test_samples):\n",
    "        #prepare input data\n",
    "        input_data = X_test[i:i+1].astype(np.float32)\n",
    "        interpreter.set_tensor(input_details[0]['index'],input_data)\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        #Get Output\n",
    "        output_data =  interpreter.get_tensor(output_details[0]['index'])\n",
    "        predicted = np.argmax(output_data)\n",
    "        if predicted == y_test[i]:\n",
    "            correct+=1\n",
    "    accuracy  =float(correct/test_samples)\n",
    "    print(f\"TFLite Accuracy(first{test_samples} samples):{accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "def save_model_for_deployment(model, project_path, history = None):\n",
    "    \"\"\"Save model in various formats for deployment\"\"\"\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"Saving Models for Deployment\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    #Save Keras model\n",
    "    keras_path = project_path/\"models\"/\"checkpoints\"/\"har_model.h5\"\n",
    "    keras_path.parent.mkdir(parents = True,exist_ok = True)\n",
    "    model.save(keras_path)\n",
    "    print(f\"Keras model saved to :{keras_path}\")\n",
    "\n",
    "    #Save training history if available\n",
    "    if history is not None:\n",
    "        history_path = project_path/\"models\"/\"checkpoints\"/\"training_history.json\"\n",
    "        history_dict ={\n",
    "            \"accuracy\":[float(x) for x in history.history[\"accuracy\"]],\n",
    "            \"val_accuracy\":[float(x) for x in history.history[\"val_accuracy\"]],\n",
    "            \"loss\":[float(x) for x in history.history[\"loss\"]],\n",
    "            \"val_loss\":[float(x) for x in history.history[\"val_loss\"]]\n",
    "        }\n",
    "        history_path.write_text(json.dumps(history_dict, indent = 2, cls = NumpyEncoder))\n",
    "        print(f\"Training history saved:{history_path}\")\n",
    "    return keras_path\n",
    "\n",
    "def create_deployment_report(project_path,model_params,inference_time,test_accuracy,tflite_accuracy,tflite_size_KB):\n",
    "    \"\"\"Create a deployment readiness report\"\"\"\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(\"Deployment Readiness Report\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    #Ensure all values are Python native types\n",
    "    model_params = int(model_params)\n",
    "    inference_time = float(inference_time)\n",
    "    test_accuracy = float(test_accuracy)\n",
    "    tflite_accuracy = float(tflite_accuracy)\n",
    "    tflite_size_KB = float(tflite_size_KB)\n",
    "\n",
    "    #Calculate constraints\n",
    "    size_check = model_params / 1024<= 2048\n",
    "    time_check = inference_time < 100\n",
    "    meets_all  = size_check and time_check\n",
    "\n",
    "    report = {\n",
    "        \"deployment_status\":\"READY\" if meets_all else \"NEED OPTIMIZATION\",\n",
    "        \"model_statistics\": {\n",
    "        \"parameters\":model_params,\n",
    "        \"fp32_size_KB\":float(model_params*4 / 1024),\n",
    "        \"int8_size_KB\":float(model_params/1024),\n",
    "        \"tflite_size_KB\":tflite_size_KB\n",
    "    },\n",
    "        \"performance_metrics\" : {\n",
    "            \"test_accuracy\":test_accuracy,\n",
    "            \"tflie_accuracy\":tflite_accuracy,\n",
    "            \"inference_time_ms\":inference_time\n",
    "            \n",
    "        },\n",
    "        \"edge_constrain_check\":{\n",
    "            \"max_size_2mb\":bool(size_check),\n",
    "            \"max_inference_100ms\":bool(time_check),\n",
    "            \"meets_all_constraints\":bool(meets_all)\n",
    "        },\n",
    "        \"deployment_files\":{\n",
    "            \"keras_model\":str(project_path/\"models\"/\"checkpoints\"/\"har_model.h5\"),\n",
    "            \"tflie_model\":str(project_path/\"config\"/\"project.json\")\n",
    "        }\n",
    "    }\n",
    "    #Print Summary\n",
    "    print(f\"DEPLOYMENT SUMMARY\")\n",
    "    print(f\"Model Size:{model_params / 1024:.1f}KB(INT8)\")\n",
    "    print(f\"Inference Summary:{inference_time:.2f} ms\")\n",
    "    print(f\"Test Accuracy:{test_acc:.4f}\")\n",
    "    print(f\"TFLite Accuracy:{tflite_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\n Edge Constraints\")\n",
    "    print(f\"2MB SIZE:\"\"pass\" if size_check else f\"2MB SIZE:\"\"fail\")\n",
    "    print(f\"100ms inference:\"\"pass\" if time_check else f\"100ms inference:\"\"fail\")\n",
    "    if meets_all:\n",
    "        print(f\"Ready for edge deployment\")\n",
    "    else:\n",
    "        print(\"Further optimization needed\")\n",
    "\n",
    "    #save report\n",
    "    report_path = project_path/\"deployment\"/\"deployment_report.json\"\n",
    "    report_path.write_text(json.dumps(report,indent =2,cls = NumpyEncoder))\n",
    "    print(f\"\\n Full Report Saved to: {report_path}\")\n",
    "    return report\n",
    "\n",
    "#========== MAIN EXECUTION ===========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        #step1:Create Project\n",
    "        print(\"=\"*60)\n",
    "        print(\"Step 1: Creating Edge AI Project Structure\")\n",
    "        print(\"=\"*60)\n",
    "        project_path = create_edge_project()\n",
    "\n",
    "        #step 2:Download the dataset\n",
    "        print(\"=\"*60)\n",
    "        print(\"Step 2: Downloading the necessary dataset from the given url\")\n",
    "        print(\"=\"*60)\n",
    "        file_name = \"human+activity+recognition+using+smartphones.zip\"\n",
    "        file_path = project_path/\"data\"/\"raw\"/file_name\n",
    "        if not file_path.exists():\n",
    "            url = \"https://archive.ics.uci.edu/static/public/240/human+activity+recognition+using+smartphones.zip\"\n",
    "            print(f\"Downloading dataset...\")\n",
    "            response = requests.get(url,timeout = 10)\n",
    "            with open(file_path,\"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"dataset downloaded to:{file_path}\")\n",
    "\n",
    "        else:\n",
    "            print(\"dataset already exists\")\n",
    "\n",
    "        # Step 3 to be done: extracting data from zip folder: for now, this is being done manually\n",
    "\n",
    "        #step 4 : loading the data\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Loading Dataset..\")\n",
    "        print(\"=\"*60)\n",
    "        data_dir = project_path/\"data\"/\"raw\"\n",
    "        (X_train,y_train),(X_test,y_test)  =load_har_data_uci_format(data_dir)\n",
    "        \n",
    "        #Step 5: Create and analyze the model\n",
    "        create_dataset_metadata(data_dir)\n",
    "        cleanup_dataset(data_dir)\n",
    "\n",
    "        #Step 6: Create and analyze model\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Creating and analyzing the model\")\n",
    "        print(\"=\"*60)\n",
    "        data_dir = project_path/\"data\"/\"raw\"\n",
    "        model = create_tiny_ann()\n",
    "        model.summary()\n",
    "\n",
    "        #Step 7 : Train the model\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Train Model\")\n",
    "        print(\"=\"*60)\n",
    "        history,trained_model = train_model(model,X_train,y_train,X_test,y_test,epochs = 50)\n",
    "\n",
    "        #Step 8 : Evaluate the model\n",
    "\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Model Evaluation\")\n",
    "        print(\"=\"*60)\n",
    "        test_acc, test_loss = evaluate_model(trained_model, X_test,y_test)\n",
    "\n",
    "        #Step 9: Edge Performance Analysis\n",
    "\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Edge Performance Analysis\")\n",
    "        print(\"=\"*60)\n",
    "        model_params, inference_time = analyze_edge_performance(trained_model, X_train[:100])\n",
    "\n",
    "        #Step 10 :Save Models\n",
    "\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Save Model\")\n",
    "        print(\"=\"*60)\n",
    "        save_model_for_deployment(trained_model,project_path,history)\n",
    "\n",
    "        #Step 11: Quantize for Edge\n",
    "\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Quantize Model for Edge\")\n",
    "        print(\"=\"*60)\n",
    "        tflite_model, tflite_path = quantize_model_for_edge(trained_model, X_train, project_path)\n",
    "\n",
    "\n",
    "        #Step 12: Test TFLite Model\n",
    "\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Testing TFLite Model\")\n",
    "        print(\"=\"*60)\n",
    "        if tflite_path and tflite_path.exists():\n",
    "            tflite_accuracy  =test_tflite_model(tflite_path, X_test,y_test)\n",
    "            tflite_size_KB = tflite_path.stat().st_size/1024\n",
    "        else:\n",
    "            print(\"TFLite model not created\")\n",
    "            tflite_accuracy = 0.0\n",
    "            tflite_size_KB = 0.0\n",
    "\n",
    "        #Step 13: Create Deployment Report\n",
    "\n",
    "        print(\"\\n\"+\"=\"*60)\n",
    "        print(\"Creating Deployment Report\")\n",
    "        print(\"=\"*60)\n",
    "        report = create_deployment_report(project_path,model_params,inference_time,test_acc,tflite_accuracy,tflite_size_KB)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EDGE AI PIPELINE COMPLETED..\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\n Edge AI Model for motion detection is available at: {project_path.absolute()}\")\n",
    "        print(\"\\nModel files created:\")\n",
    "        print(f\"  âœ“ Keras model: models/checkpoints/har_model.h5\")\n",
    "        if tflite_path and tflite_path.exists():\n",
    "            print(f\"  âœ“ TFLite model: models/tflite/har_edge_model_int8.tflite\")\n",
    "        print(f\"  âœ“ Config: config/project.json\")\n",
    "        print(f\"  âœ“ Report: deployment/deployment_report.json\")\n",
    "        print(f\"  âœ“ Dataset metadata: data/raw/dataset_metadata.json\")\n",
    "        \n",
    "        print(\"\\nğŸ“Š Model Performance Summary:\")\n",
    "        print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"  TFLite Accuracy: {tflite_accuracy:.4f}\")\n",
    "        print(f\"  Model Size: {model_params / 1024:.1f} KB (INT8)\")\n",
    "        print(f\"  Inference Time: {inference_time:.2f} ms\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ERROR: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"Please check the error above and try again.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb56241-b4c4-4c3d-8783-a8993dc2326f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
